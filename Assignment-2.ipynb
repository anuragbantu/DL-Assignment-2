{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11270754,"sourceType":"datasetVersion","datasetId":7045380}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, random_split\n","metadata":{"id":"FlhfR94ImdB4","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:29.129542Z","iopub.execute_input":"2025-04-12T06:43:29.129950Z","iopub.status.idle":"2025-04-12T06:43:29.134968Z","shell.execute_reply.started":"2025-04-12T06:43:29.129901Z","shell.execute_reply":"2025-04-12T06:43:29.133920Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{"id":"PxyWosUnfe8D"}},{"cell_type":"code","source":"# We resize the input images, convert them to tensors and normalize them to [-1,1]\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),   # Resize images to 128x128\n    transforms.ToTensor(),           # Convert images to PyTorch tensors\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n])\n\n# Augmented transform\naugmented_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])","metadata":{"id":"ErpjVuMdrjxz","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:29.136294Z","iopub.execute_input":"2025-04-12T06:43:29.136653Z","iopub.status.idle":"2025-04-12T06:43:29.156746Z","shell.execute_reply.started":"2025-04-12T06:43:29.136629Z","shell.execute_reply":"2025-04-12T06:43:29.155690Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"train_dir = '/kaggle/input/inaturalist-1/inaturalist_12K/train'\ntest_dir = '/kaggle/input/inaturalist-1/inaturalist_12K/val'\n\ntrain_dataset = ImageFolder(root=train_dir, transform=transform)\ntest_dataset = ImageFolder(root=test_dir, transform=transform)\n\n","metadata":{"id":"pKB-VYi1WrXU","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:29.158782Z","iopub.execute_input":"2025-04-12T06:43:29.159146Z","iopub.status.idle":"2025-04-12T06:43:36.618125Z","shell.execute_reply.started":"2025-04-12T06:43:29.159109Z","shell.execute_reply":"2025-04-12T06:43:36.617314Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Load the full training dataset\nfull_train_dataset = ImageFolder(root=train_dir, transform=transform)\n\n# Calculate split sizes\nval_size = int(0.2 * len(full_train_dataset))\ntrain_size = len(full_train_dataset) - val_size\n\n# Split the dataset\ntrain_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\n# Loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# Load the test set\ntest_dataset = ImageFolder(root=test_dir, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:36.619333Z","iopub.execute_input":"2025-04-12T06:43:36.619630Z","iopub.status.idle":"2025-04-12T06:43:36.700135Z","shell.execute_reply.started":"2025-04-12T06:43:36.619606Z","shell.execute_reply":"2025-04-12T06:43:36.699364Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"#Make sure every class is represented equally in validation data\n\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Get original dataset labels (from the subset indices)\nval_targets = [full_train_dataset.targets[i] for i in val_dataset.indices]\n\n# Count frequency of each class\nval_class_counts = Counter(val_targets)\n\nprint(sorted(val_class_counts.items()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:36.700985Z","iopub.execute_input":"2025-04-12T06:43:36.701215Z","iopub.status.idle":"2025-04-12T06:43:36.707097Z","shell.execute_reply.started":"2025-04-12T06:43:36.701187Z","shell.execute_reply":"2025-04-12T06:43:36.706060Z"}},"outputs":[{"name":"stdout","text":"[(0, 197), (1, 216), (2, 206), (3, 188), (4, 206), (5, 192), (6, 189), (7, 190), (8, 210), (9, 205)]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"num_classes = len(full_train_dataset.classes)\n\nclass CNN(nn.Module):\n    def __init__(self, num_classes, conv_filters=[96, 256, 384, 384, 256], kernel_sizes=[3, 3, 3, 3, 3], activation_fn=F.relu, fc_units=[1024],dropout=[0.0],\n        use_batchnorm=False):\n        super(CNN, self).__init__()\n        \n        \n        assert len(conv_filters) == len(kernel_sizes), \"conv_filters and kernel_sizes must be the same length\"\n        \n        self.activation_fn = activation_fn\n        self.use_batchnorm = use_batchnorm\n\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        \n        self.conv_layers = nn.ModuleList()\n        self.batchnorm_layers = nn.ModuleList()\n\n\n        in_channels = 3  \n        for out_channels, kernel_size in zip(conv_filters, kernel_sizes):\n            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2))\n            \n            if use_batchnorm:\n                self.batchnorm_layers.append(nn.BatchNorm2d(out_channels))\n            \n            in_channels = out_channels  \n        \n        \n        # Dynamically compute the flattened size \n        with torch.no_grad():\n            dummy_input = torch.zeros(1, 3, 128, 128)  \n            x = dummy_input\n            for idx, conv in enumerate(self.conv_layers):\n                x = conv(x)\n                if self.use_batchnorm:\n                    x = self.batchnorm_layers[idx](x)\n                x = self.activation_fn(x)\n                x = self.pool(x)\n            self.flattened_size = x.view(1, -1).size(1)\n\n\n        # Only one dense layer\n        self.fc = nn.Linear(self.flattened_size, fc_units[0])\n        self.dropout = nn.Dropout(dropout)\n\n        # Final output layer\n        self.fc_out = nn.Linear(fc_units[0], num_classes)\n    \n    def forward(self, x):\n        for idx, conv in enumerate(self.conv_layers):\n            x = conv(x)\n            if self.use_batchnorm:\n                x = self.batchnorm_layers[idx](x)\n            x = self.activation_fn(x)\n            x = self.pool(x)\n\n        x = torch.flatten(x, 1)\n        \n        x = self.activation_fn(self.fc(x))\n        x = self.dropout(x)\n        x = self.fc_out(x)\n        return x","metadata":{"id":"dXBF1OERkdBl","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:36.708001Z","iopub.execute_input":"2025-04-12T06:43:36.708209Z","iopub.status.idle":"2025-04-12T06:43:36.720198Z","shell.execute_reply.started":"2025-04-12T06:43:36.708190Z","shell.execute_reply":"2025-04-12T06:43:36.719271Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#model = CNN(num_classes, conv_filters=[64, 128, 256, 256, 128], kernel_sizes=[5, 3, 3, 3, 3], activation_fn=F.leaky_relu, fc_units=[512, 256]).to(device)","metadata":{"id":"FIqibLkyII8H","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:36.721061Z","iopub.execute_input":"2025-04-12T06:43:36.721367Z","iopub.status.idle":"2025-04-12T06:43:36.737857Z","shell.execute_reply.started":"2025-04-12T06:43:36.721338Z","shell.execute_reply":"2025-04-12T06:43:36.736956Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"\"\"\"\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(5):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    # Training phase\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Forward pass\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Calculate training metrics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n\n    # Compute training loss and accuracy for the epoch\n    train_loss = running_loss / len(train_loader)\n    train_acc = 100 * correct_train / total_train\n\n    # --- Validation phase ---\n    model.eval()  # Set model to evaluation mode\n    correct_val = 0\n    total_val = 0\n\n    with torch.no_grad():  # Disable gradient computation\n        for val_inputs, val_labels in val_loader:\n            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n            val_outputs = model(val_inputs)\n            _, val_predicted = torch.max(val_outputs.data, 1)\n            total_val += val_labels.size(0)\n            correct_val += (val_predicted == val_labels).sum().item()\n\n    # Compute validation accuracy for the epoch\n    val_acc = 100 * correct_val / total_val  \n    model.train()  # Switch back to training mode\n\n    # Print epoch-wise training and validation metrics\n    print(f\"Epoch {epoch+1}/{15}\")\n    print(f\"  Training Loss: {train_loss:.4f} | Training Accuracy: {train_acc:.2f}%\")\n    print(f\"  Validation Accuracy: {val_acc:.2f}%\\n\")\n\n\n# --- Test phase (after training) ---\nmodel.eval()\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for test_inputs, test_labels in test_loader:\n        test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n        test_outputs = model(test_inputs)\n        _, test_predicted = torch.max(test_outputs.data, 1)\n        total_test += test_labels.size(0)\n        correct_test += (test_predicted == test_labels).sum().item()\n\ntest_acc = 100 * correct_test / total_test\nprint(f\"Final Test Accuracy: {test_acc:.2f}%\")\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:36.740223Z","iopub.execute_input":"2025-04-12T06:43:36.740533Z","iopub.status.idle":"2025-04-12T06:43:36.751747Z","shell.execute_reply.started":"2025-04-12T06:43:36.740509Z","shell.execute_reply":"2025-04-12T06:43:36.750810Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'\\n# Loss function and optimizer\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\\n\\n# Training loop\\nfor epoch in range(5):\\n    model.train()\\n    running_loss = 0.0\\n    correct_train = 0\\n    total_train = 0\\n\\n    # Training phase\\n    for inputs, labels in train_loader:\\n        inputs, labels = inputs.to(device), labels.to(device)\\n\\n        # Forward pass\\n        optimizer.zero_grad()\\n        outputs = model(inputs)\\n        loss = criterion(outputs, labels)\\n\\n        # Backward pass and optimize\\n        loss.backward()\\n        optimizer.step()\\n\\n        # Calculate training metrics\\n        running_loss += loss.item()\\n        _, predicted = torch.max(outputs.data, 1)\\n        total_train += labels.size(0)\\n        correct_train += (predicted == labels).sum().item()\\n\\n    # Compute training loss and accuracy for the epoch\\n    train_loss = running_loss / len(train_loader)\\n    train_acc = 100 * correct_train / total_train\\n\\n    # --- Validation phase ---\\n    model.eval()  # Set model to evaluation mode\\n    correct_val = 0\\n    total_val = 0\\n\\n    with torch.no_grad():  # Disable gradient computation\\n        for val_inputs, val_labels in val_loader:\\n            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\\n            val_outputs = model(val_inputs)\\n            _, val_predicted = torch.max(val_outputs.data, 1)\\n            total_val += val_labels.size(0)\\n            correct_val += (val_predicted == val_labels).sum().item()\\n\\n    # Compute validation accuracy for the epoch\\n    val_acc = 100 * correct_val / total_val  \\n    model.train()  # Switch back to training mode\\n\\n    # Print epoch-wise training and validation metrics\\n    print(f\"Epoch {epoch+1}/{15}\")\\n    print(f\"  Training Loss: {train_loss:.4f} | Training Accuracy: {train_acc:.2f}%\")\\n    print(f\"  Validation Accuracy: {val_acc:.2f}%\\n\")\\n\\n\\n# --- Test phase (after training) ---\\nmodel.eval()\\ncorrect_test = 0\\ntotal_test = 0\\nwith torch.no_grad():\\n    for test_inputs, test_labels in test_loader:\\n        test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\\n        test_outputs = model(test_inputs)\\n        _, test_predicted = torch.max(test_outputs.data, 1)\\n        total_test += test_labels.size(0)\\n        correct_test += (test_predicted == test_labels).sum().item()\\n\\ntest_acc = 100 * correct_test / total_test\\nprint(f\"Final Test Accuracy: {test_acc:.2f}%\")\\n'"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"import wandb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:36.753225Z","iopub.execute_input":"2025-04-12T06:43:36.753516Z","iopub.status.idle":"2025-04-12T06:43:36.769443Z","shell.execute_reply.started":"2025-04-12T06:43:36.753487Z","shell.execute_reply":"2025-04-12T06:43:36.768223Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"activation_map = {\n    'relu': F.relu,\n    'gelu': F.gelu,\n    'silu': F.silu,\n    'mish': F.mish\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:36.770456Z","iopub.execute_input":"2025-04-12T06:43:36.770814Z","iopub.status.idle":"2025-04-12T06:43:36.786410Z","shell.execute_reply.started":"2025-04-12T06:43:36.770779Z","shell.execute_reply":"2025-04-12T06:43:36.785368Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def train(config=None):\n    with wandb.init(config=config) as run:\n        config = wandb.config\n        \n        run.name = f\"filters={config.conv_filters}_act={config.activation_fn}_aug={config.use_augmentation}_bn={config.use_batchnorm}_dropout={config.dropout}\"\n        run.save()\n\n        # Choose transform for training\n        if config.use_augmentation:\n            train_transform = augmented_transform\n        else:\n            train_transform = transform\n\n        # Load full dataset with chosen transform\n        full_train_dataset = ImageFolder(train_dir, transform=train_transform)\n\n        # Split into train and val\n        val_size = int(0.2 * len(full_train_dataset))\n        train_size = len(full_train_dataset) - val_size\n        train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\n        # Set val transform to basic (no augmentation)\n        val_dataset.dataset.transform = transform\n\n        # Data loaders\n        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n\n\n        # Model instantiation \n        model = CNN(\n                  num_classes=num_classes,\n                  conv_filters=config.conv_filters,\n                  kernel_sizes=[3] * len(config.conv_filters), #why 3?\n                  activation_fn=activation_map[config.activation_fn],\n                  fc_units=config.fc_units,\n                  dropout=config.dropout,\n                  use_batchnorm=config.use_batchnorm\n              ).to(device)\n\n        # Loss and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n\n        for epoch in range(config.epochs):\n            model.train()\n            running_loss = 0.0\n            correct_train = 0\n            total_train = 0\n\n            for inputs, labels in train_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total_train += labels.size(0)\n                correct_train += (predicted == labels).sum().item()\n\n            train_loss = running_loss / len(train_loader)\n            train_acc = 100 * correct_train / total_train\n\n            # Validation phase\n            model.eval()\n            correct_val = 0\n            total_val = 0\n            with torch.no_grad():\n                for val_inputs, val_labels in val_loader:\n                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n                    val_outputs = model(val_inputs)\n                    _, val_predicted = torch.max(val_outputs.data, 1)\n                    total_val += val_labels.size(0)\n                    correct_val += (val_predicted == val_labels).sum().item()\n\n            val_acc = 100 * correct_val / total_val\n            model.train()\n\n            print(f\"Epoch {epoch+1}/{config.epochs}\")\n            print(f\"  Training Loss: {train_loss:.4f} | Training Accuracy: {train_acc:.2f}%\")\n            print(f\"  Validation Accuracy: {val_acc:.2f}%\\n\")\n\n            # Log metrics to W&B\n            wandb.log({\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_acc\": val_acc\n            })\n\n        # Test accuracy (do not log to W&B)\n        model.eval()\n        correct_test = 0\n        total_test = 0\n        with torch.no_grad():\n            for test_inputs, test_labels in test_loader:\n                test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n                test_outputs = model(test_inputs)\n                _, test_preds = torch.max(test_outputs.data, 1)\n                total_test += test_labels.size(0)\n                correct_test += (test_preds == test_labels).sum().item()\n\n        test_acc = 100 * correct_test / total_test\n        print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:36.787512Z","iopub.execute_input":"2025-04-12T06:43:36.787875Z","iopub.status.idle":"2025-04-12T06:43:36.802017Z","shell.execute_reply.started":"2025-04-12T06:43:36.787839Z","shell.execute_reply":"2025-04-12T06:43:36.801146Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',  \n    'metric': {\n        'name': 'val_acc',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'lr': {\n            'values': [0.001, 0.0005, 0.0001]\n        },\n        'epochs': {\n            'values': [5,10]\n        },\n        'conv_filters': {\n            'values': [\n                [32, 64, 128, 256, 512],\n                [64, 128, 256, 512, 1024],\n                [32, 64, 64, 128, 128],\n                [128, 128, 128, 128, 128],\n                [1024,512,256,128,64]\n            ]\n        },\n        'activation_fn': {\n            'values': ['relu', 'gelu', 'silu', 'mish']\n        },\n        'fc_units': {\n            'values': [[256],[512],[1024],]\n        },\n        'batch_size': {'values': [32, 64]},\n        'use_batchnorm': {'values': [True, False]},\n        'use_augmentation': {'values': [True, False]},\n        'dropout': {'values': [0,0.1,0.2,0.3,0.5]}\n\n    }\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:36.802884Z","iopub.execute_input":"2025-04-12T06:43:36.803114Z","iopub.status.idle":"2025-04-12T06:43:36.819425Z","shell.execute_reply.started":"2025-04-12T06:43:36.803094Z","shell.execute_reply":"2025-04-12T06:43:36.818562Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"wandb.login(key='af7d7cf29d8954a13afb06c7a0d0c196c36ac51b')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:36.820485Z","iopub.execute_input":"2025-04-12T06:43:36.820740Z","iopub.status.idle":"2025-04-12T06:43:37.058444Z","shell.execute_reply.started":"2025-04-12T06:43:36.820719Z","shell.execute_reply":"2025-04-12T06:43:37.057316Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma24m003\u001b[0m (\u001b[33mma24m003-iit-madras\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"# Create sweep\nsweep_id = wandb.sweep(sweep_config, project=\"inaturalist-hyperparam-tuning\")\n\n# Launch sweep agents\nwandb.agent(sweep_id, function=train, count=10)  # runs 10 experiments\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:43:37.059789Z","iopub.execute_input":"2025-04-12T06:43:37.060113Z","iopub.status.idle":"2025-04-12T06:48:25.687626Z","shell.execute_reply.started":"2025-04-12T06:43:37.060086Z","shell.execute_reply":"2025-04-12T06:48:25.684773Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: a5t1v75b\nSweep URL: https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/a5t1v75b\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: drtnu49b with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [64, 128, 256, 512, 1024]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_units: [512]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250412_064343-drtnu49b</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/drtnu49b' target=\"_blank\">usual-sweep-1</a></strong> to <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/a5t1v75b' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/a5t1v75b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/a5t1v75b' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/a5t1v75b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/drtnu49b' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/drtnu49b</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n  Training Loss: 2.5526 | Training Accuracy: 14.88%\n  Validation Accuracy: 16.71%\n\nEpoch 2/10\n  Training Loss: 2.2263 | Training Accuracy: 16.82%\n  Validation Accuracy: 19.21%\n\nEpoch 3/10\n  Training Loss: 2.1983 | Training Accuracy: 19.26%\n  Validation Accuracy: 20.91%\n\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"> Best model evaluation on test dataset","metadata":{}},{"cell_type":"code","source":"best_config = {\n    \"conv_filters\": [32,64,64,128,128],\n    \"fc_units\": [512],\n    \"dropout\": 0.3,\n    \"activation_fn\": \"mish\",\n    \"use_batchnorm\": True,\n    \"lr\": 0.0005,\n    \"batch_size\": 64,\n    \"epochs\": 10,\n    'use_augmentation' : False\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:09:55.079316Z","iopub.execute_input":"2025-04-12T07:09:55.079766Z","iopub.status.idle":"2025-04-12T07:09:55.084682Z","shell.execute_reply.started":"2025-04-12T07:09:55.079718Z","shell.execute_reply":"2025-04-12T07:09:55.083558Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"test_dataset = ImageFolder(test_dir, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:49:44.303315Z","iopub.execute_input":"2025-04-12T06:49:44.303741Z","iopub.status.idle":"2025-04-12T06:49:44.338621Z","shell.execute_reply.started":"2025-04-12T06:49:44.303710Z","shell.execute_reply":"2025-04-12T06:49:44.337587Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"model = CNN(\n    num_classes=num_classes,\n    conv_filters=best_config[\"conv_filters\"],\n    kernel_sizes=[3] * len(best_config[\"conv_filters\"]),\n    activation_fn=activation_map[best_config[\"activation_fn\"]],\n    fc_units=best_config[\"fc_units\"],\n    dropout=best_config[\"dropout\"],\n    use_batchnorm=best_config[\"use_batchnorm\"]\n).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:10:15.811229Z","iopub.execute_input":"2025-04-12T07:10:15.811602Z","iopub.status.idle":"2025-04-12T07:10:15.909233Z","shell.execute_reply.started":"2025-04-12T07:10:15.811572Z","shell.execute_reply":"2025-04-12T07:10:15.908155Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"train(config=best_config)\n#torch.save(model.state_dict(), \"best_model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:10:20.438001Z","iopub.execute_input":"2025-04-12T07:10:20.438328Z","iopub.status.idle":"2025-04-12T07:17:42.530773Z","shell.execute_reply.started":"2025-04-12T07:10:20.438303Z","shell.execute_reply":"2025-04-12T07:17:42.529272Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250412_071020-drtnu49b</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/drtnu49b' target=\"_blank\">filters=[64, 128, 256, 512, 1024]_act=gelu_aug=False_bn=True_dropout=0.3</a></strong> to <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/a5t1v75b' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/a5t1v75b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/a5t1v75b' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/a5t1v75b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/drtnu49b' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/drtnu49b</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n  Training Loss: 2.6732 | Training Accuracy: 13.30%\n  Validation Accuracy: 15.26%\n\nEpoch 2/10\n  Training Loss: 2.2346 | Training Accuracy: 17.06%\n  Validation Accuracy: 17.71%\n\nEpoch 3/10\n  Training Loss: 2.2087 | Training Accuracy: 18.23%\n  Validation Accuracy: 20.96%\n\nEpoch 4/10\n  Training Loss: 2.1736 | Training Accuracy: 19.89%\n  Validation Accuracy: 24.86%\n\nEpoch 5/10\n  Training Loss: 2.1629 | Training Accuracy: 19.91%\n  Validation Accuracy: 22.66%\n\nEpoch 6/10\n  Training Loss: 2.1516 | Training Accuracy: 19.69%\n  Validation Accuracy: 23.71%\n\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"<ipython-input-33-359b46b9a3fe>\", line 60, in train\n    running_loss += loss.item()\nKeyboardInterrupt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▅▆███</td></tr><tr><td>train_loss</td><td>█▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▅█▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>19.6875</td></tr><tr><td>train_loss</td><td>2.1516</td></tr><tr><td>val_acc</td><td>23.71186</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">filters=[64, 128, 256, 512, 1024]_act=gelu_aug=False_bn=True_dropout=0.3</strong> at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/drtnu49b' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/drtnu49b</a><br> View project at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250412_071020-drtnu49b/logs</code>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-0c1d050637b7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#torch.save(model.state_dict(), \"best_model.pth\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-359b46b9a3fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mtotal_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":50},{"cell_type":"code","source":"torch.save(model.state_dict(), \"best_model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:01:35.878536Z","iopub.execute_input":"2025-04-12T07:01:35.878994Z","iopub.status.idle":"2025-04-12T07:01:35.977395Z","shell.execute_reply.started":"2025-04-12T07:01:35.878953Z","shell.execute_reply":"2025-04-12T07:01:35.976486Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"best_model.pth\"))\nmodel.eval()\n# Evaluate on test\n\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ntest_acc = 100 * correct / total\nprint(f\"Test Accuracy: {test_acc:.2f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:01:40.598670Z","iopub.execute_input":"2025-04-12T07:01:40.598980Z","iopub.status.idle":"2025-04-12T07:02:01.704164Z","shell.execute_reply.started":"2025-04-12T07:01:40.598956Z","shell.execute_reply":"2025-04-12T07:02:01.702744Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-46-23df2785acce>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"best_model.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 10.00%\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"#Make sure every class is represented equally in validation data\n\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Get original dataset labels (from the subset indices)\ntest_targets = [full_train_dataset.targets[i] for i in test_dataset.indices]\n\n# Count frequency of each class\ntest_class_counts = Counter(test_targets)\n\nprint(sorted(val_class_counts.items()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:06:38.063934Z","iopub.execute_input":"2025-04-12T07:06:38.064309Z","iopub.status.idle":"2025-04-12T07:06:38.096021Z","shell.execute_reply.started":"2025-04-12T07:06:38.064278Z","shell.execute_reply":"2025-04-12T07:06:38.094749Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-5a29b55da441>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Get original dataset labels (from the subset indices)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfull_train_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Count frequency of each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'ImageFolder' object has no attribute 'indices'"],"ename":"AttributeError","evalue":"'ImageFolder' object has no attribute 'indices'","output_type":"error"}],"execution_count":47},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Load the model\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\nmodel.eval()\n\n# Get a batch of test images\nimages, labels = next(iter(test_loader))\nimages, labels = images.to(device), labels.to(device)\n\n# Get model predictions\nwith torch.no_grad():\n    outputs = model(images)\n    _, predicted = torch.max(outputs, 1)\n\n# Move tensors to CPU for visualization\nimages = images.cpu()\nlabels = labels.cpu()\npredicted = predicted.cpu()\n\n# Class names\nclass_names = test_dataset.classes\n\n# Function to unnormalize images\ndef imshow(img):\n    img = img / 2 + 0.5  # unnormalize\n    npimg = img.numpy()\n    return np.transpose(npimg, (1, 2, 0))\n\n# Plot the 10x3 grid\nfig, axes = plt.subplots(10, 3, figsize=(12, 30))\nfig.suptitle('🔍 Predictions on Test Images', fontsize=22, fontweight='bold', y=1.02)\n\nfor i in range(30):\n    row = i // 3\n    col = i % 3\n    ax = axes[row, col]\n    img = imshow(images[i])\n    ax.imshow(img)\n    true_label = class_names[labels[i]]\n    pred_label = class_names[predicted[i]]\n    \n    if true_label == pred_label:\n        title_color = 'green'\n    else:\n        title_color = 'red'\n    \n    ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", color=title_color, fontsize=10, fontweight='bold')\n    ax.axis('off')\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.95)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:49:29.508480Z","iopub.status.idle":"2025-04-12T06:49:29.508901Z","shell.execute_reply":"2025-04-12T06:49:29.508708Z"}},"outputs":[],"execution_count":null}]}