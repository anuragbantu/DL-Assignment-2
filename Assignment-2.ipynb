{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11270754,"sourceType":"datasetVersion","datasetId":7045380}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, random_split\nimport json","metadata":{"id":"FlhfR94ImdB4","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:11:59.601621Z","iopub.execute_input":"2025-04-16T06:11:59.601872Z","iopub.status.idle":"2025-04-16T06:12:05.566163Z","shell.execute_reply.started":"2025-04-16T06:11:59.601838Z","shell.execute_reply":"2025-04-16T06:12:05.565473Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Data preparation and Preprocessing","metadata":{"id":"PxyWosUnfe8D"}},{"cell_type":"code","source":"# We resize the input images, convert them to tensors and normalize them to [-1,1]\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),   # Resize images to 128x128\n    transforms.ToTensor(),           # Convert images to PyTorch tensors\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n])\n\n# Augmented transform\naugmented_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])","metadata":{"id":"ErpjVuMdrjxz","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:12:05.566857Z","iopub.execute_input":"2025-04-16T06:12:05.567134Z","iopub.status.idle":"2025-04-16T06:12:05.572291Z","shell.execute_reply.started":"2025-04-16T06:12:05.567117Z","shell.execute_reply":"2025-04-16T06:12:05.571440Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_dir = '/kaggle/input/inaturalist-1/inaturalist_12K/train'\ntest_dir = '/kaggle/input/inaturalist-1/inaturalist_12K/val'\n\ntrain_dataset = ImageFolder(root=train_dir, transform=transform)\ntest_dataset = ImageFolder(root=test_dir, transform=transform)\n\n","metadata":{"id":"pKB-VYi1WrXU","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:12:05.573208Z","iopub.execute_input":"2025-04-16T06:12:05.573500Z","iopub.status.idle":"2025-04-16T06:12:31.408939Z","shell.execute_reply.started":"2025-04-16T06:12:05.573471Z","shell.execute_reply":"2025-04-16T06:12:31.408242Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load the full training dataset\nfull_train_dataset = ImageFolder(root=train_dir, transform=transform)\n\n# Calculate split sizes\nval_size = int(0.2 * len(full_train_dataset))\ntrain_size = len(full_train_dataset) - val_size\n\n# Split the dataset\ntrain_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\n# Loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# Load the test set\ntest_dataset = ImageFolder(root=test_dir, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:12:31.411145Z","iopub.execute_input":"2025-04-16T06:12:31.411359Z","iopub.status.idle":"2025-04-16T06:12:33.771809Z","shell.execute_reply.started":"2025-04-16T06:12:31.411341Z","shell.execute_reply":"2025-04-16T06:12:33.770922Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#Make sure every class is represented equally in validation data\n\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Get original dataset labels (from the subset indices)\nval_targets = [full_train_dataset.targets[i] for i in val_dataset.indices]\n\n# Count frequency of each class\nval_class_counts = Counter(val_targets)\n\nprint(sorted(val_class_counts.items()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:12:33.773106Z","iopub.execute_input":"2025-04-16T06:12:33.773333Z","iopub.status.idle":"2025-04-16T06:12:33.778868Z","shell.execute_reply.started":"2025-04-16T06:12:33.773315Z","shell.execute_reply":"2025-04-16T06:12:33.777929Z"}},"outputs":[{"name":"stdout","text":"[(0, 200), (1, 184), (2, 201), (3, 223), (4, 161), (5, 204), (6, 212), (7, 198), (8, 211), (9, 205)]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"num_classes = len(full_train_dataset.classes)\n\nclass CNN(nn.Module):\n    def __init__(self, num_classes, conv_filters=[96, 256, 384, 384, 256], kernel_sizes=[3, 3, 3, 3, 3], activation_fn=F.relu, fc_units=[1024],dropout=[0.0],\n        use_batchnorm=False):\n        super(CNN, self).__init__()\n        \n        \n        assert len(conv_filters) == len(kernel_sizes), \"conv_filters and kernel_sizes must be the same length\"\n        \n        self.activation_fn = activation_fn\n        self.use_batchnorm = use_batchnorm\n\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        \n        self.conv_layers = nn.ModuleList()\n        self.batchnorm_layers = nn.ModuleList()\n\n\n        in_channels = 3  \n        for out_channels, kernel_size in zip(conv_filters, kernel_sizes):\n            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2))\n            \n            if use_batchnorm:\n                self.batchnorm_layers.append(nn.BatchNorm2d(out_channels))\n            \n            in_channels = out_channels  \n        \n        \n        # Dynamically compute the flattened size \n        with torch.no_grad():\n            dummy_input = torch.zeros(1, 3, 128, 128)  \n            x = dummy_input\n            for idx, conv in enumerate(self.conv_layers):\n                x = conv(x)\n                if self.use_batchnorm:\n                    x = self.batchnorm_layers[idx](x)\n                x = self.activation_fn(x)\n                x = self.pool(x)\n            self.flattened_size = x.view(1, -1).size(1)\n\n\n        # Only one dense layer\n        self.fc = nn.Linear(self.flattened_size, fc_units[0])\n        self.dropout = nn.Dropout(dropout)\n\n        # Final output layer\n        self.fc_out = nn.Linear(fc_units[0], num_classes)\n    \n    def forward(self, x):\n        for idx, conv in enumerate(self.conv_layers):\n            x = conv(x)\n            if self.use_batchnorm:\n                x = self.batchnorm_layers[idx](x)\n            x = self.activation_fn(x)\n            x = self.pool(x)\n\n        x = torch.flatten(x, 1)\n        \n        x = self.activation_fn(self.fc(x))\n        x = self.dropout(x)\n        x = self.fc_out(x)\n        return x","metadata":{"id":"dXBF1OERkdBl","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:12:33.779654Z","iopub.execute_input":"2025-04-16T06:12:33.779913Z","iopub.status.idle":"2025-04-16T06:12:33.797215Z","shell.execute_reply.started":"2025-04-16T06:12:33.779894Z","shell.execute_reply":"2025-04-16T06:12:33.796591Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#model = CNN(num_classes, conv_filters=[64, 128, 256, 256, 128], kernel_sizes=[5, 3, 3, 3, 3], activation_fn=F.leaky_relu, fc_units=[512, 256]).to(device)","metadata":{"id":"FIqibLkyII8H","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:12:33.798140Z","iopub.execute_input":"2025-04-16T06:12:33.798426Z","iopub.status.idle":"2025-04-16T06:12:33.886070Z","shell.execute_reply.started":"2025-04-16T06:12:33.798399Z","shell.execute_reply":"2025-04-16T06:12:33.885216Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import wandb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:12:33.886821Z","iopub.execute_input":"2025-04-16T06:12:33.887047Z","iopub.status.idle":"2025-04-16T06:12:36.027263Z","shell.execute_reply.started":"2025-04-16T06:12:33.887028Z","shell.execute_reply":"2025-04-16T06:12:36.026500Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"activation_map = {\n    'relu': F.relu,\n    'gelu': F.gelu,\n    'silu': F.silu,\n    'mish': F.mish\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:12:36.028068Z","iopub.execute_input":"2025-04-16T06:12:36.028315Z","iopub.status.idle":"2025-04-16T06:12:36.032348Z","shell.execute_reply.started":"2025-04-16T06:12:36.028293Z","shell.execute_reply":"2025-04-16T06:12:36.031350Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def train(config=None):\n    with wandb.init(config=config) as run:\n        config = wandb.config\n        \n        run.name = f\"filters={config.conv_filters}_act={config.activation_fn}_aug={config.use_augmentation}_bn={config.use_batchnorm}_dropout={config.dropout}\"\n        run.save()\n\n        # Choose transform for training\n        if config.use_augmentation:\n            train_transform = augmented_transform\n        else:\n            train_transform = transform\n\n        # Load full dataset with chosen transform\n        full_train_dataset = ImageFolder(train_dir, transform=train_transform)\n\n        # Split into train and val\n        val_size = int(0.2 * len(full_train_dataset))\n        train_size = len(full_train_dataset) - val_size\n        train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\n        # Set val transform to basic (no augmentation)\n        val_dataset.dataset.transform = transform\n\n        # Data loaders\n        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n\n\n        # Model instantiation \n        model = CNN(\n                  num_classes=num_classes,\n                  conv_filters=config.conv_filters,\n                  kernel_sizes=[3] * len(config.conv_filters), #why 3?\n                  activation_fn=activation_map[config.activation_fn],\n                  fc_units=config.fc_units,\n                  dropout=config.dropout,\n                  use_batchnorm=config.use_batchnorm\n              ).to(device)\n\n        # Loss and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n\n        \n        best_val_acc = 0.0\n\n        for epoch in range(config.epochs):\n            model.train()\n            running_loss = 0.0\n            correct_train = 0\n            total_train = 0\n\n            for inputs, labels in train_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total_train += labels.size(0)\n                correct_train += (predicted == labels).sum().item()\n\n            train_loss = running_loss / len(train_loader)\n            train_acc = 100 * correct_train / total_train\n\n            # Validation phase\n            model.eval()\n            correct_val = 0\n            total_val = 0\n            with torch.no_grad():\n                for val_inputs, val_labels in val_loader:\n                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n                    val_outputs = model(val_inputs)\n                    _, val_predicted = torch.max(val_outputs.data, 1)\n                    total_val += val_labels.size(0)\n                    correct_val += (val_predicted == val_labels).sum().item()\n\n            val_acc = 100 * correct_val / total_val\n            model.train()\n\n            print(f\"Epoch {epoch+1}/{config.epochs}\")\n            print(f\"  Training Loss: {train_loss:.4f} | Training Accuracy: {train_acc:.2f}%\")\n            print(f\"  Validation Accuracy: {val_acc:.2f}%\\n\")\n\n            # Save best model and config\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                torch.save(model.state_dict(), \"best_model.pth\")\n                with open(\"best_config.json\", \"w\") as f:\n                    json.dump(dict(config), f)\n\n            # Log metrics to W&B\n            wandb.log({\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_acc\": val_acc\n            })\n\n        # Test accuracy (do not log to W&B)\n        model.eval()\n        correct_test = 0\n        total_test = 0\n        with torch.no_grad():\n            for test_inputs, test_labels in test_loader:\n                test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n                test_outputs = model(test_inputs)\n                _, test_preds = torch.max(test_outputs.data, 1)\n                total_test += test_labels.size(0)\n                correct_test += (test_preds == test_labels).sum().item()\n\n        test_acc = 100 * correct_test / total_test\n        print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:12:36.033340Z","iopub.execute_input":"2025-04-16T06:12:36.033655Z","iopub.status.idle":"2025-04-16T06:12:36.058777Z","shell.execute_reply.started":"2025-04-16T06:12:36.033620Z","shell.execute_reply":"2025-04-16T06:12:36.057982Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',  \n    'metric': {\n        'name': 'val_acc',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'lr': {\n            'values': [0.001, 0.0005, 0.0001]\n        },\n        'epochs': {\n            'values': [5,10]\n        },\n        'conv_filters': {\n            'values': [\n                [32, 64, 128, 256, 512],\n                [64, 128, 256, 512, 1024],\n                [32, 64, 64, 128, 128],\n                [128, 128, 128, 128, 128],\n                [1024,512,256,128,64],\n                [512,256,128,64,32]\n            ]\n        },\n        'activation_fn': {\n            'values': ['relu', 'gelu', 'silu', 'mish']\n        },\n        'fc_units': {\n            'values': [[256],[512],[1024],]\n        },\n        'batch_size': {'values': [32, 64]},\n        'use_batchnorm': {'values': [True, False]},\n        'use_augmentation': {'values': [True, False]},\n        'dropout': {'values': [0,0.1,0.2,0.3,0.5]}\n\n    }\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:12:36.059622Z","iopub.execute_input":"2025-04-16T06:12:36.059842Z","iopub.status.idle":"2025-04-16T06:12:36.076237Z","shell.execute_reply.started":"2025-04-16T06:12:36.059823Z","shell.execute_reply":"2025-04-16T06:12:36.075606Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"wandb.login(key='af7d7cf29d8954a13afb06c7a0d0c196c36ac51b')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:12:36.077035Z","iopub.execute_input":"2025-04-16T06:12:36.077324Z","iopub.status.idle":"2025-04-16T06:12:42.264232Z","shell.execute_reply.started":"2025-04-16T06:12:36.077297Z","shell.execute_reply":"2025-04-16T06:12:42.263317Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma24m003\u001b[0m (\u001b[33mma24m003-iit-madras\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Create sweep\nsweep_id = wandb.sweep(sweep_config, project=\"inaturalist-hyperparam-tuning\")\n\n# Launch sweep agents\nwandb.agent(sweep_id, function=train, count=5)  # runs 10 experiments\n\nwandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:12:42.266966Z","iopub.execute_input":"2025-04-16T06:12:42.267313Z","execution_failed":"2025-04-16T07:29:11.276Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Create sweep with ID: o5rum79w\nSweep URL: https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9z0j3wnj with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: relu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [32, 64, 128, 256, 512]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_units: [256]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_061248-9z0j3wnj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/9z0j3wnj' target=\"_blank\">dulcet-sweep-1</a></strong> to <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/9z0j3wnj' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/9z0j3wnj</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n  Training Loss: 2.4003 | Training Accuracy: 17.52%\n  Validation Accuracy: 22.01%\n\nEpoch 2/10\n  Training Loss: 2.0901 | Training Accuracy: 24.48%\n  Validation Accuracy: 27.86%\n\nEpoch 3/10\n  Training Loss: 2.0117 | Training Accuracy: 26.96%\n  Validation Accuracy: 29.41%\n\nEpoch 4/10\n  Training Loss: 1.9463 | Training Accuracy: 30.18%\n  Validation Accuracy: 31.17%\n\nEpoch 5/10\n  Training Loss: 1.8825 | Training Accuracy: 32.85%\n  Validation Accuracy: 32.12%\n\nEpoch 6/10\n  Training Loss: 1.8339 | Training Accuracy: 35.39%\n  Validation Accuracy: 34.02%\n\nEpoch 7/10\n  Training Loss: 1.7731 | Training Accuracy: 36.96%\n  Validation Accuracy: 32.27%\n\nEpoch 8/10\n  Training Loss: 1.7385 | Training Accuracy: 38.27%\n  Validation Accuracy: 33.32%\n\nEpoch 9/10\n  Training Loss: 1.6761 | Training Accuracy: 40.64%\n  Validation Accuracy: 36.12%\n\nEpoch 10/10\n  Training Loss: 1.6088 | Training Accuracy: 43.48%\n  Validation Accuracy: 33.27%\n\nFinal Test Accuracy: 32.25%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▃▄▄▅▆▆▇▇█</td></tr><tr><td>train_loss</td><td>█▅▅▄▃▃▂▂▂▁</td></tr><tr><td>val_acc</td><td>▁▄▅▆▆▇▆▇█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>43.475</td></tr><tr><td>train_loss</td><td>1.60875</td></tr><tr><td>val_acc</td><td>33.26663</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">filters=[32, 64, 128, 256, 512]_act=relu_aug=False_bn=True_dropout=0</strong> at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/9z0j3wnj' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/9z0j3wnj</a><br> View project at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_061248-9z0j3wnj/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kvz6fzf8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [64, 128, 256, 512, 1024]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_units: [1024]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_062403-kvz6fzf8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/kvz6fzf8' target=\"_blank\">easy-sweep-2</a></strong> to <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/kvz6fzf8' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/kvz6fzf8</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n  Training Loss: 2.7328 | Training Accuracy: 17.49%\n  Validation Accuracy: 20.51%\n\nEpoch 2/5\n  Training Loss: 2.0992 | Training Accuracy: 24.56%\n  Validation Accuracy: 24.21%\n\nEpoch 3/5\n  Training Loss: 2.0426 | Training Accuracy: 26.19%\n  Validation Accuracy: 25.61%\n\nEpoch 4/5\n  Training Loss: 1.9784 | Training Accuracy: 29.74%\n  Validation Accuracy: 29.16%\n\nEpoch 5/5\n  Training Loss: 1.9341 | Training Accuracy: 31.19%\n  Validation Accuracy: 28.71%\n\nFinal Test Accuracy: 29.10%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▅▅▇█</td></tr><tr><td>train_loss</td><td>█▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>31.1875</td></tr><tr><td>train_loss</td><td>1.9341</td></tr><tr><td>val_acc</td><td>28.71436</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">filters=[64, 128, 256, 512, 1024]_act=gelu_aug=False_bn=True_dropout=0</strong> at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/kvz6fzf8' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/kvz6fzf8</a><br> View project at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_062403-kvz6fzf8/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vmlqgaq3 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [512, 256, 128, 64, 32]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_units: [1024]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_063030-vmlqgaq3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/vmlqgaq3' target=\"_blank\">stellar-sweep-3</a></strong> to <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/vmlqgaq3' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/vmlqgaq3</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n  Training Loss: 2.1605 | Training Accuracy: 21.31%\n  Validation Accuracy: 25.51%\n\nEpoch 2/5\n  Training Loss: 2.0264 | Training Accuracy: 27.89%\n  Validation Accuracy: 26.16%\n\nEpoch 3/5\n  Training Loss: 1.9489 | Training Accuracy: 31.09%\n  Validation Accuracy: 30.82%\n\nEpoch 4/5\n  Training Loss: 1.8760 | Training Accuracy: 33.56%\n  Validation Accuracy: 33.02%\n\nEpoch 5/5\n  Training Loss: 1.7972 | Training Accuracy: 37.01%\n  Validation Accuracy: 30.67%\n\nFinal Test Accuracy: 31.20%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▄▅▆█</td></tr><tr><td>train_loss</td><td>█▅▄▃▁</td></tr><tr><td>val_acc</td><td>▁▂▆█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>37.0125</td></tr><tr><td>train_loss</td><td>1.79724</td></tr><tr><td>val_acc</td><td>30.66533</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">filters=[512, 256, 128, 64, 32]_act=silu_aug=False_bn=False_dropout=0.2</strong> at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/vmlqgaq3' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/vmlqgaq3</a><br> View project at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_063030-vmlqgaq3/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wgd9zilg with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [1024, 512, 256, 128, 64]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_units: [512]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_063633-wgd9zilg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/wgd9zilg' target=\"_blank\">zany-sweep-4</a></strong> to <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/o5rum79w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/wgd9zilg' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/wgd9zilg</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n  Training Loss: 2.0575 | Training Accuracy: 26.45%\n  Validation Accuracy: 29.16%\n\nEpoch 2/5\n  Training Loss: 1.8811 | Training Accuracy: 33.71%\n  Validation Accuracy: 31.62%\n\nEpoch 3/5\n  Training Loss: 1.7812 | Training Accuracy: 37.21%\n  Validation Accuracy: 32.32%\n\nEpoch 4/5\n  Training Loss: 1.6675 | Training Accuracy: 41.45%\n  Validation Accuracy: 37.92%\n\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"> Best model evaluation on test dataset","metadata":{}},{"cell_type":"code","source":"# Load config and model\nwith open(\"best_config.json\", \"r\") as f:\n    best_config = json.load(f)\n\nmodel = CNN(\n    num_classes=num_classes,\n    conv_filters=best_config[\"conv_filters\"],\n    kernel_sizes=[3] * len(best_config[\"conv_filters\"]),\n    activation_fn=activation_map[best_config[\"activation_fn\"]],\n    fc_units=best_config[\"fc_units\"],\n    dropout=best_config[\"dropout\"],\n    use_batchnorm=best_config[\"use_batchnorm\"]\n).to(device)\n\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\nmodel.eval()\n\n# Evaluate on test set\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for test_inputs, test_labels in test_loader:\n        test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n        outputs = model(test_inputs)\n        _, preds = torch.max(outputs, 1)\n        correct_test += (preds == test_labels).sum().item()\n        total_test += test_labels.size(0)\n\ntest_acc = 100 * correct_test / total_test\nprint(f\"Loaded Best Model Test Accuracy: {test_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-16T07:29:11.276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_config","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-16T07:29:11.277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs, labels = next(iter(test_loader))\ninputs, labels = inputs.to(device), labels.to(device)\noutputs = model(inputs)\npreds = outputs.argmax(dim=1)\nprint(\"Predicted:\", preds[:20].cpu().numpy())\nprint(\"Actual   :\", labels[:20].cpu().numpy())\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-16T07:29:11.277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"best_model.pth\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-16T07:29:11.277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Load the model\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\nmodel.eval()\n\n# Get a batch of test images\nimages, labels = next(iter(test_loader))\nimages, labels = images.to(device), labels.to(device)\n\n# Get model predictions\nwith torch.no_grad():\n    outputs = model(images)\n    _, predicted = torch.max(outputs, 1)\n\n# Move tensors to CPU for visualization\nimages = images.cpu()\nlabels = labels.cpu()\npredicted = predicted.cpu()\n\n# Class names\nclass_names = test_dataset.classes\n\n# Function to unnormalize images\ndef imshow(img):\n    img = img / 2 + 0.5  # unnormalize\n    npimg = img.numpy()\n    return np.transpose(npimg, (1, 2, 0))\n\n# Plot the 10x3 grid\nfig, axes = plt.subplots(10, 3, figsize=(12, 30))\nfig.suptitle(' Predictions on Test Images', fontsize=22, fontweight='bold', y=1.02)\n\nfor i in range(30):\n    row = i // 3\n    col = i % 3\n    ax = axes[row, col]\n    img = imshow(images[i])\n    ax.imshow(img)\n    true_label = class_names[labels[i]]\n    pred_label = class_names[predicted[i]]\n    \n    if true_label == pred_label:\n        title_color = 'green'\n    else:\n        title_color = 'red'\n    \n    ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", color=title_color, fontsize=10, fontweight='bold')\n    ax.axis('off')\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.95)\nwandb.init(project=\"inaturalist-hyperparam-tuning\", name=\"best_model_pred_visualization\", mode=\"online\") \n\nwandb.log({\"Test Predictions Grid\": wandb.Image(fig)})\n\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-16T07:29:11.277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_config = {\n    \"conv_filters\": [64, 128, 256, 512, 1024],\n    \"fc_units\": [256],\n    \"dropout\": 0.5,\n    \"activation_fn\": \"mish\",\n    \"use_batchnorm\": True,\n    \"lr\": 0.0001,\n    \"batch_size\": 64,\n    \"epochs\": 10,\n    'use_augmentation' : True\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:46:21.699229Z","iopub.execute_input":"2025-04-16T11:46:21.701498Z","iopub.status.idle":"2025-04-16T11:46:21.709073Z","shell.execute_reply.started":"2025-04-16T11:46:21.701467Z","shell.execute_reply":"2025-04-16T11:46:21.708012Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"model = CNN(\n    num_classes=num_classes,\n    conv_filters=best_config[\"conv_filters\"],\n    kernel_sizes=[3] * len(best_config[\"conv_filters\"]),\n    activation_fn=activation_map[best_config[\"activation_fn\"]],\n    fc_units=best_config[\"fc_units\"],\n    dropout=best_config[\"dropout\"],\n    use_batchnorm=best_config[\"use_batchnorm\"]\n).to(device)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-16T07:29:11.277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(config=best_config)\nmodel.eval()\n# Evaluate on test\n\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ntest_acc = 100 * correct / total\nprint(f\"Test Accuracy: {test_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-16T07:29:11.277Z"}},"outputs":[],"execution_count":null}]}