{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11270754,"sourceType":"datasetVersion","datasetId":7045380}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, random_split\n","metadata":{"id":"FlhfR94ImdB4","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:36.775340Z","iopub.execute_input":"2025-04-08T08:40:36.775657Z","iopub.status.idle":"2025-04-08T08:40:43.291921Z","shell.execute_reply.started":"2025-04-08T08:40:36.775603Z","shell.execute_reply":"2025-04-08T08:40:43.291248Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{"id":"PxyWosUnfe8D"}},{"cell_type":"code","source":"# We resize the input images, convert them to tensors and normalize them to [-1,1]\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),   # Resize images to 128x128\n    transforms.ToTensor(),           # Convert images to PyTorch tensors\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n])\n\n# Augmented transform\naugmented_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])","metadata":{"id":"ErpjVuMdrjxz","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:43.292647Z","iopub.execute_input":"2025-04-08T08:40:43.292968Z","iopub.status.idle":"2025-04-08T08:40:43.297926Z","shell.execute_reply.started":"2025-04-08T08:40:43.292948Z","shell.execute_reply":"2025-04-08T08:40:43.297019Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_dir = '/kaggle/input/inaturalist-1/inaturalist_12K/train'\ntest_dir = '/kaggle/input/inaturalist-1/inaturalist_12K/val'\n\ntrain_dataset = ImageFolder(root=train_dir, transform=transform)\ntest_dataset = ImageFolder(root=test_dir, transform=transform)\n\n","metadata":{"id":"pKB-VYi1WrXU","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:43.299002Z","iopub.execute_input":"2025-04-08T08:40:43.299299Z","iopub.status.idle":"2025-04-08T08:40:49.450935Z","shell.execute_reply.started":"2025-04-08T08:40:43.299268Z","shell.execute_reply":"2025-04-08T08:40:49.450023Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load the full training dataset\nfull_train_dataset = ImageFolder(root=train_dir, transform=transform)\n\n# Calculate split sizes\nval_size = int(0.2 * len(full_train_dataset))\ntrain_size = len(full_train_dataset) - val_size\n\n# Split the dataset\ntrain_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\n# Loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# Load the test set\ntest_dataset = ImageFolder(root=test_dir, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:49.453065Z","iopub.execute_input":"2025-04-08T08:40:49.453316Z","iopub.status.idle":"2025-04-08T08:40:51.165182Z","shell.execute_reply.started":"2025-04-08T08:40:49.453297Z","shell.execute_reply":"2025-04-08T08:40:51.164514Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#Make sure every class is represented equally in validation data\n\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Get original dataset labels (from the subset indices)\nval_targets = [full_train_dataset.targets[i] for i in val_dataset.indices]\n\n# Count frequency of each class\nval_class_counts = Counter(val_targets)\n\nprint(sorted(val_class_counts.items()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:51.166278Z","iopub.execute_input":"2025-04-08T08:40:51.166563Z","iopub.status.idle":"2025-04-08T08:40:51.171698Z","shell.execute_reply.started":"2025-04-08T08:40:51.166540Z","shell.execute_reply":"2025-04-08T08:40:51.170921Z"}},"outputs":[{"name":"stdout","text":"[(0, 203), (1, 185), (2, 213), (3, 200), (4, 194), (5, 189), (6, 186), (7, 228), (8, 205), (9, 196)]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"num_classes = len(full_train_dataset.classes)\n\nclass CNN(nn.Module):\n    def __init__(self, num_classes, conv_filters=[96, 256, 384, 384, 256], kernel_sizes=[3, 3, 3, 3, 3], activation_fn=F.relu, fc_units=[1024],dropout=[0.0],\n        use_batchnorm=False):\n        super(CNN, self).__init__()\n        \n        \n        assert len(conv_filters) == len(kernel_sizes), \"conv_filters and kernel_sizes must be the same length\"\n        \n        self.activation_fn = activation_fn\n        self.use_batchnorm = use_batchnorm\n\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        \n        self.conv_layers = nn.ModuleList()\n        self.batchnorm_layers = nn.ModuleList()\n\n\n        in_channels = 3  \n        for out_channels, kernel_size in zip(conv_filters, kernel_sizes):\n            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2))\n            \n            if use_batchnorm:\n                self.batchnorm_layers.append(nn.BatchNorm2d(out_channels))\n            \n            in_channels = out_channels  \n        \n        \n        # Dynamically compute the flattened size \n        with torch.no_grad():\n            dummy_input = torch.zeros(1, 3, 128, 128)  \n            x = dummy_input\n            for idx, conv in enumerate(self.conv_layers):\n                x = conv(x)\n                if self.use_batchnorm:\n                    x = self.batchnorm_layers[idx](x)\n                x = self.activation_fn(x)\n                x = self.pool(x)\n            self.flattened_size = x.view(1, -1).size(1)\n\n\n        # Only one dense layer\n        self.fc = nn.Linear(self.flattened_size, fc_units[0])\n        self.dropout = nn.Dropout(dropout)\n\n        # Final output layer\n        self.fc_out = nn.Linear(fc_units[0], num_classes)\n    \n    def forward(self, x):\n        for idx, conv in enumerate(self.conv_layers):\n            x = conv(x)\n            if self.use_batchnorm:\n                x = self.batchnorm_layers[idx](x)\n            x = self.activation_fn(x)\n            x = self.pool(x)\n\n        x = torch.flatten(x, 1)\n        \n        x = self.activation_fn(self.fc(x))\n        x = self.dropout(x)\n        x = self.fc_out(x)\n        return x","metadata":{"id":"dXBF1OERkdBl","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:51.172390Z","iopub.execute_input":"2025-04-08T08:40:51.172615Z","iopub.status.idle":"2025-04-08T08:40:51.259024Z","shell.execute_reply.started":"2025-04-08T08:40:51.172596Z","shell.execute_reply":"2025-04-08T08:40:51.257987Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#model = CNN(num_classes, conv_filters=[64, 128, 256, 256, 128], kernel_sizes=[5, 3, 3, 3, 3], activation_fn=F.leaky_relu, fc_units=[512, 256]).to(device)","metadata":{"id":"FIqibLkyII8H","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:51.259915Z","iopub.execute_input":"2025-04-08T08:40:51.260181Z","iopub.status.idle":"2025-04-08T08:40:51.369634Z","shell.execute_reply.started":"2025-04-08T08:40:51.260161Z","shell.execute_reply":"2025-04-08T08:40:51.368946Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\"\"\"\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(5):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    # Training phase\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Forward pass\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Calculate training metrics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n\n    # Compute training loss and accuracy for the epoch\n    train_loss = running_loss / len(train_loader)\n    train_acc = 100 * correct_train / total_train\n\n    # --- Validation phase ---\n    model.eval()  # Set model to evaluation mode\n    correct_val = 0\n    total_val = 0\n\n    with torch.no_grad():  # Disable gradient computation\n        for val_inputs, val_labels in val_loader:\n            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n            val_outputs = model(val_inputs)\n            _, val_predicted = torch.max(val_outputs.data, 1)\n            total_val += val_labels.size(0)\n            correct_val += (val_predicted == val_labels).sum().item()\n\n    # Compute validation accuracy for the epoch\n    val_acc = 100 * correct_val / total_val  \n    model.train()  # Switch back to training mode\n\n    # Print epoch-wise training and validation metrics\n    print(f\"Epoch {epoch+1}/{15}\")\n    print(f\"  Training Loss: {train_loss:.4f} | Training Accuracy: {train_acc:.2f}%\")\n    print(f\"  Validation Accuracy: {val_acc:.2f}%\\n\")\n\n\n# --- Test phase (after training) ---\nmodel.eval()\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for test_inputs, test_labels in test_loader:\n        test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n        test_outputs = model(test_inputs)\n        _, test_predicted = torch.max(test_outputs.data, 1)\n        total_test += test_labels.size(0)\n        correct_test += (test_predicted == test_labels).sum().item()\n\ntest_acc = 100 * correct_test / total_test\nprint(f\"Final Test Accuracy: {test_acc:.2f}%\")\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:51.371136Z","iopub.execute_input":"2025-04-08T08:40:51.371379Z","iopub.status.idle":"2025-04-08T08:40:51.390830Z","shell.execute_reply.started":"2025-04-08T08:40:51.371357Z","shell.execute_reply":"2025-04-08T08:40:51.390031Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\n# Loss function and optimizer\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\\n\\n# Training loop\\nfor epoch in range(5):\\n    model.train()\\n    running_loss = 0.0\\n    correct_train = 0\\n    total_train = 0\\n\\n    # Training phase\\n    for inputs, labels in train_loader:\\n        inputs, labels = inputs.to(device), labels.to(device)\\n\\n        # Forward pass\\n        optimizer.zero_grad()\\n        outputs = model(inputs)\\n        loss = criterion(outputs, labels)\\n\\n        # Backward pass and optimize\\n        loss.backward()\\n        optimizer.step()\\n\\n        # Calculate training metrics\\n        running_loss += loss.item()\\n        _, predicted = torch.max(outputs.data, 1)\\n        total_train += labels.size(0)\\n        correct_train += (predicted == labels).sum().item()\\n\\n    # Compute training loss and accuracy for the epoch\\n    train_loss = running_loss / len(train_loader)\\n    train_acc = 100 * correct_train / total_train\\n\\n    # --- Validation phase ---\\n    model.eval()  # Set model to evaluation mode\\n    correct_val = 0\\n    total_val = 0\\n\\n    with torch.no_grad():  # Disable gradient computation\\n        for val_inputs, val_labels in val_loader:\\n            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\\n            val_outputs = model(val_inputs)\\n            _, val_predicted = torch.max(val_outputs.data, 1)\\n            total_val += val_labels.size(0)\\n            correct_val += (val_predicted == val_labels).sum().item()\\n\\n    # Compute validation accuracy for the epoch\\n    val_acc = 100 * correct_val / total_val  \\n    model.train()  # Switch back to training mode\\n\\n    # Print epoch-wise training and validation metrics\\n    print(f\"Epoch {epoch+1}/{15}\")\\n    print(f\"  Training Loss: {train_loss:.4f} | Training Accuracy: {train_acc:.2f}%\")\\n    print(f\"  Validation Accuracy: {val_acc:.2f}%\\n\")\\n\\n\\n# --- Test phase (after training) ---\\nmodel.eval()\\ncorrect_test = 0\\ntotal_test = 0\\nwith torch.no_grad():\\n    for test_inputs, test_labels in test_loader:\\n        test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\\n        test_outputs = model(test_inputs)\\n        _, test_predicted = torch.max(test_outputs.data, 1)\\n        total_test += test_labels.size(0)\\n        correct_test += (test_predicted == test_labels).sum().item()\\n\\ntest_acc = 100 * correct_test / total_test\\nprint(f\"Final Test Accuracy: {test_acc:.2f}%\")\\n'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import wandb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:51.391539Z","iopub.execute_input":"2025-04-08T08:40:51.391820Z","iopub.status.idle":"2025-04-08T08:40:53.531708Z","shell.execute_reply.started":"2025-04-08T08:40:51.391799Z","shell.execute_reply":"2025-04-08T08:40:53.531078Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"activation_map = {\n    'relu': F.relu,\n    'gelu': F.gelu,\n    'silu': F.silu,\n    'mish': F.mish\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:53.532390Z","iopub.execute_input":"2025-04-08T08:40:53.532605Z","iopub.status.idle":"2025-04-08T08:40:53.536139Z","shell.execute_reply.started":"2025-04-08T08:40:53.532577Z","shell.execute_reply":"2025-04-08T08:40:53.535212Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def train(config=None):\n    with wandb.init(config=config) as run:\n        config = wandb.config\n        \n        run.name = f\"filters={config.conv_filters}_act={config.activation_fn}_aug={config.use_augmentation}_bn={config.use_batchnorm}_dropout={config.dropout}\"\n        run.save()\n\n        # Choose transform for training\n        if config.use_augmentation:\n            train_transform = augmented_transform\n        else:\n            train_transform = transform\n\n        # Load full dataset with chosen transform\n        full_train_dataset = ImageFolder(train_dir, transform=train_transform)\n\n        # Split into train and val\n        val_size = int(0.2 * len(full_train_dataset))\n        train_size = len(full_train_dataset) - val_size\n        train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\n        # Set val transform to basic (no augmentation)\n        val_dataset.dataset.transform = transform\n\n        # Data loaders\n        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n\n\n        # Model instantiation \n        model = CNN(\n                  num_classes=num_classes,\n                  conv_filters=config.conv_filters,\n                  kernel_sizes=[3] * len(config.conv_filters),\n                  activation_fn=activation_map[config.activation_fn],\n                  fc_units=config.fc_units,\n                  dropout=config.dropout,\n                  use_batchnorm=config.use_batchnorm\n              ).to(device)\n\n        # Loss and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n\n        for epoch in range(config.epochs):\n            model.train()\n            running_loss = 0.0\n            correct_train = 0\n            total_train = 0\n\n            for inputs, labels in train_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total_train += labels.size(0)\n                correct_train += (predicted == labels).sum().item()\n\n            train_loss = running_loss / len(train_loader)\n            train_acc = 100 * correct_train / total_train\n\n            # Validation phase\n            model.eval()\n            correct_val = 0\n            total_val = 0\n            with torch.no_grad():\n                for val_inputs, val_labels in val_loader:\n                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n                    val_outputs = model(val_inputs)\n                    _, val_predicted = torch.max(val_outputs.data, 1)\n                    total_val += val_labels.size(0)\n                    correct_val += (val_predicted == val_labels).sum().item()\n\n            val_acc = 100 * correct_val / total_val\n            model.train()\n\n            print(f\"Epoch {epoch+1}/{config.epochs}\")\n            print(f\"  Training Loss: {train_loss:.4f} | Training Accuracy: {train_acc:.2f}%\")\n            print(f\"  Validation Accuracy: {val_acc:.2f}%\\n\")\n\n            # Log metrics to W&B\n            wandb.log({\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_acc\": val_acc\n            })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:53.536940Z","iopub.execute_input":"2025-04-08T08:40:53.537223Z","iopub.status.idle":"2025-04-08T08:40:53.559973Z","shell.execute_reply.started":"2025-04-08T08:40:53.537197Z","shell.execute_reply":"2025-04-08T08:40:53.559079Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',  \n    'metric': {\n        'name': 'val_acc',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'lr': {\n            'values': [0.001, 0.0005, 0.0001]\n        },\n        'epochs': {\n            'values': [5,10]\n        },\n        'conv_filters': {\n            'values': [\n                [32, 64, 128, 256, 512],\n                [64, 128, 256, 512, 1024],\n                [32, 64, 64, 128, 128],\n                [128, 128, 128, 128, 128],\n                [1024,512,256,128,64]\n            ]\n        },\n        'activation_fn': {\n            'values': ['relu', 'gelu', 'silu', 'mish']\n        },\n        'fc_units': {\n            'values': [[512], [1024], [512, 256]]\n        },\n        'batch_size': {'values': [32, 64]},\n        'use_batchnorm': {'values': [True, False]},\n        'use_augmentation': {'values': [True, False]},\n        'dropout': {'values': [0,0.2,0.3,0.5]}\n\n    }\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:53.560759Z","iopub.execute_input":"2025-04-08T08:40:53.560989Z","iopub.status.idle":"2025-04-08T08:40:53.578644Z","shell.execute_reply.started":"2025-04-08T08:40:53.560956Z","shell.execute_reply":"2025-04-08T08:40:53.577892Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"wandb.login(key='af7d7cf29d8954a13afb06c7a0d0c196c36ac51b')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:40:53.580954Z","iopub.execute_input":"2025-04-08T08:40:53.581145Z","iopub.status.idle":"2025-04-08T08:41:00.009377Z","shell.execute_reply.started":"2025-04-08T08:40:53.581128Z","shell.execute_reply":"2025-04-08T08:41:00.008705Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma24m003\u001b[0m (\u001b[33mma24m003-iit-madras\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Create sweep\nsweep_id = wandb.sweep(sweep_config, project=\"inaturalist-hyperparam-tuning\")\n\n# Launch sweep agents\nwandb.agent(sweep_id, function=train, count=10)  # runs 10 experiments\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:41:00.010289Z","iopub.execute_input":"2025-04-08T08:41:00.010724Z","execution_failed":"2025-04-08T13:26:40.103Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: ascb14im\nSweep URL: https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: toun9m1s with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [1024, 512, 256, 128, 64]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_units: [1024]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250408_084107-toun9m1s</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/toun9m1s' target=\"_blank\">super-sweep-1</a></strong> to <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/toun9m1s' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/toun9m1s</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n  Training Loss: 2.1606 | Training Accuracy: 20.77%\n  Validation Accuracy: 24.56%\n\nEpoch 2/5\n  Training Loss: 2.0051 | Training Accuracy: 28.84%\n  Validation Accuracy: 27.96%\n\nEpoch 3/5\n  Training Loss: 1.8824 | Training Accuracy: 33.26%\n  Validation Accuracy: 31.42%\n\nEpoch 4/5\n  Training Loss: 1.7846 | Training Accuracy: 37.24%\n  Validation Accuracy: 33.27%\n\nEpoch 5/5\n  Training Loss: 1.6892 | Training Accuracy: 41.00%\n  Validation Accuracy: 34.17%\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▄▅▇█</td></tr><tr><td>train_loss</td><td>█▆▄▂▁</td></tr><tr><td>val_acc</td><td>▁▃▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>41</td></tr><tr><td>train_loss</td><td>1.68922</td></tr><tr><td>val_acc</td><td>34.16708</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">filters=[1024, 512, 256, 128, 64]_act=mish_aug=False_bn=False_dropout=0.3</strong> at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/toun9m1s' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/toun9m1s</a><br> View project at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250408_084107-toun9m1s/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: amu3z2kn with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [32, 64, 64, 128, 128]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_units: [512]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250408_085342-amu3z2kn</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/amu3z2kn' target=\"_blank\">crimson-sweep-2</a></strong> to <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/amu3z2kn' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/amu3z2kn</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n  Training Loss: 2.1841 | Training Accuracy: 19.69%\n  Validation Accuracy: 26.96%\n\nEpoch 2/10\n  Training Loss: 2.0147 | Training Accuracy: 28.55%\n  Validation Accuracy: 28.66%\n\nEpoch 3/10\n  Training Loss: 1.9171 | Training Accuracy: 32.12%\n  Validation Accuracy: 30.77%\n\nEpoch 4/10\n  Training Loss: 1.8434 | Training Accuracy: 35.16%\n  Validation Accuracy: 32.67%\n\nEpoch 5/10\n  Training Loss: 1.7688 | Training Accuracy: 37.52%\n  Validation Accuracy: 34.12%\n\nEpoch 6/10\n  Training Loss: 1.6684 | Training Accuracy: 41.30%\n  Validation Accuracy: 34.52%\n\nEpoch 7/10\n  Training Loss: 1.5299 | Training Accuracy: 46.26%\n  Validation Accuracy: 34.97%\n\nEpoch 8/10\n  Training Loss: 1.3432 | Training Accuracy: 53.51%\n  Validation Accuracy: 35.77%\n\nEpoch 9/10\n  Training Loss: 1.1026 | Training Accuracy: 61.27%\n  Validation Accuracy: 34.42%\n\nEpoch 10/10\n  Training Loss: 0.8137 | Training Accuracy: 72.62%\n  Validation Accuracy: 34.12%\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▂▃▃▃▄▅▅▆█</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▅▅▄▂▁</td></tr><tr><td>val_acc</td><td>▁▂▄▆▇▇▇█▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>72.625</td></tr><tr><td>train_loss</td><td>0.81369</td></tr><tr><td>val_acc</td><td>34.11706</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">filters=[32, 64, 64, 128, 128]_act=gelu_aug=True_bn=False_dropout=0.2</strong> at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/amu3z2kn' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/amu3z2kn</a><br> View project at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250408_085342-amu3z2kn/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zoq72ymd with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: relu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [128, 128, 128, 128, 128]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_units: [1024]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250408_090222-zoq72ymd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/zoq72ymd' target=\"_blank\">sweepy-sweep-3</a></strong> to <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/zoq72ymd' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/zoq72ymd</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n  Training Loss: 2.0499 | Training Accuracy: 26.38%\n  Validation Accuracy: 29.61%\n\nEpoch 2/10\n  Training Loss: 1.8539 | Training Accuracy: 34.52%\n  Validation Accuracy: 28.31%\n\nEpoch 3/10\n  Training Loss: 1.7249 | Training Accuracy: 40.08%\n  Validation Accuracy: 34.87%\n\nEpoch 4/10\n  Training Loss: 1.6088 | Training Accuracy: 44.21%\n  Validation Accuracy: 33.47%\n\nEpoch 5/10\n  Training Loss: 1.4835 | Training Accuracy: 49.01%\n  Validation Accuracy: 34.62%\n\nEpoch 6/10\n  Training Loss: 1.3505 | Training Accuracy: 54.31%\n  Validation Accuracy: 38.57%\n\nEpoch 7/10\n  Training Loss: 1.2017 | Training Accuracy: 59.96%\n  Validation Accuracy: 35.92%\n\nEpoch 8/10\n  Training Loss: 1.0141 | Training Accuracy: 67.38%\n  Validation Accuracy: 34.22%\n\nEpoch 9/10\n  Training Loss: 0.8089 | Training Accuracy: 75.40%\n  Validation Accuracy: 37.77%\n\nEpoch 10/10\n  Training Loss: 0.6180 | Training Accuracy: 82.56%\n  Validation Accuracy: 36.92%\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▂▃▃▄▄▅▆▇█</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▅▄▃▂▁</td></tr><tr><td>val_acc</td><td>▂▁▅▅▅█▆▅▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>82.5625</td></tr><tr><td>train_loss</td><td>0.61804</td></tr><tr><td>val_acc</td><td>36.91846</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">filters=[128, 128, 128, 128, 128]_act=relu_aug=True_bn=True_dropout=0</strong> at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/zoq72ymd' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/zoq72ymd</a><br> View project at: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250408_090222-zoq72ymd/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sis3m5vs with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [32, 64, 64, 128, 128]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_units: [512, 256]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250408_091135-sis3m5vs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/sis3m5vs' target=\"_blank\">floral-sweep-4</a></strong> to <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/sweeps/ascb14im</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/sis3m5vs' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/inaturalist-hyperparam-tuning/runs/sis3m5vs</a>"},"metadata":{}}],"execution_count":null}]}